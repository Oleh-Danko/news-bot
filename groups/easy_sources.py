# groups/easy_sources.py
import asyncio
import logging
import re
from typing import List, Dict, Optional
from aiohttp import ClientSession
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime, timedelta, date
from zoneinfo import ZoneInfo

log = logging.getLogger("easy_sources")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml",
}

TZ = ZoneInfo("Europe/Kyiv")

EP_BASE = "https://epravda.com.ua"
MF_BASE = "https://minfin.com.ua"

EP_PAGES = [
    "/finances",  # –±–µ—Ä–µ–º–æ –ª–∏—à–µ —Ñ—ñ–Ω–∞–Ω—Å–∏; —ñ–Ω—à—ñ —Ä–æ–∑–¥—ñ–ª–∏ –º–æ–∂—É—Ç—å –ø—ñ–¥—Ç—è–≥—É–≤–∞—Ç–∏—Å—å –ø–µ—Ä–µ—Ö—Ä–µ—Å–Ω–∏–º–∏ –ª—ñ–Ω–∫–∞–º–∏
]
MF_PAGES = [
    "/ua/news",
    "/ua/news/money-management/",
    "/ua/news/commerce/",
]

MAX_PER_PAGE = 60  # –∑–∞—Ö–∏—Å–Ω–∞ ¬´—Å—Ç–µ–ª—è¬ª –∑—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏

UA_MONTHS = {
    "—Å—ñ—á–Ω—è": 1, "–ª—é—Ç–æ–≥–æ": 2, "–±–µ—Ä–µ–∑–Ω—è": 3, "–∫–≤—ñ—Ç–Ω—è": 4, "—Ç—Ä–∞–≤–Ω—è": 5, "—á–µ—Ä–≤–Ω—è": 6,
    "–ª–∏–ø–Ω—è": 7, "—Å–µ—Ä–ø–Ω—è": 8, "–≤–µ—Ä–µ—Å–Ω—è": 9, "–∂–æ–≤—Ç–Ω—è": 10, "–ª–∏—Å—Ç–æ–ø–∞–¥–∞": 11, "–≥—Ä—É–¥–Ω—è": 12,
}

def _norm(s: str) -> str:
    return " ".join(s.split()) if isinstance(s, str) else s

def _add(items: List[Dict], title: str, href: str, date_text: str, src: str, category: str):
    if not href or not title:
        return
    items.append({
        "title": _norm(title),
        "link": href.strip(),
        "published": _norm(date_text) if date_text else "",
        "src": src,
        "category": category.strip("/") or "news",
    })

def _extract_date_from_url(url: str) -> Optional[date]:
    # minfin: /YYYY/MM/DD/
    m = re.search(r"/(20\d{2})/(\d{2})/(\d{2})/", url)
    if m:
        y, mo, d = map(int, m.groups())
        try:
            return date(y, mo, d)
        except ValueError:
            return None
    return None

def _extract_date_from_text(text: str) -> Optional[date]:
    if not text:
        return None
    # 2025-10-28
    m = re.search(r"(20\d{2})-(\d{2})-(\d{2})", text)
    if m:
        y, mo, d = map(int, m.groups())
        try:
            return date(y, mo, d)
        except ValueError:
            return None
    # ¬´28 –∂–æ–≤—Ç–Ω—è 2025¬ª
    m = re.search(r"(\d{1,2})\s+([–ê-–Ø–∞-—è–Ü—ñ–á—ó–Ñ—î“ê“ë]+)\s+(20\d{2})", text)
    if m:
        d = int(m.group(1))
        mon = UA_MONTHS.get(m.group(2).lower())
        y = int(m.group(3))
        if mon:
            try:
                return date(y, mon, d)
            except ValueError:
                return None
    return None

def _only_today_yesterday(items: List[Dict]) -> List[Dict]:
    today = datetime.now(TZ).date()
    yesterday = today - timedelta(days=1)
    out = []
    for it in items:
        d = _extract_date_from_url(it["link"]) or _extract_date_from_text(it.get("published", ""))
        if d and (d == today or d == yesterday):
            out.append(it)
    return out

async def _fetch(session: ClientSession, url: str) -> str:
    async with session.get(url, headers=HEADERS, timeout=20) as r:
        r.raise_for_status()
        return await r.text()

def _parse_epravda(html: str, path: str) -> List[Dict]:
    soup = BeautifulSoup(html, "html.parser")
    out: List[Dict] = []
    # —Ç–∏–ø–æ–≤—ñ —Ç–∞–π—Ç–ª-–ª—ñ–Ω–∫–∏
    for a in soup.select("a.item__title, article a.article__title, article a.list-item__title"):
        href = a.get("href") or ""
        if href.startswith("/"):
            href = urljoin(EP_BASE, href)
        title = a.get_text(strip=True)
        # —à—É–∫–∞—î–º–æ time/date –ø–æ—Ä—è–¥, —è–∫—â–æ —î
        date_text = ""
        parent = a.find_parent(["article", "div", "li"])
        if parent:
            dt = parent.select_one("time, .article__date, .list-item__date")
            if dt:
                date_text = dt.get_text(" ", strip=True)
        # –∫–∞—Ç–µ–≥–æ—Ä—ñ—è –∑ URL
        cat = "news"
        try:
            cat = href.split(EP_BASE)[-1].split("/")[1]
        except Exception:
            pass
        _add(out, title, href, date_text, "epravda", cat)
        if len(out) >= MAX_PER_PAGE:
            break
    return out

def _parse_minfin(html: str, path: str) -> List[Dict]:
    soup = BeautifulSoup(html, "html.parser")
    out: List[Dict] = []
    # –±–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ –ª—ñ–Ω–∫–∏ –∑ –¥–∞—Ç–æ—é —É URL
    for a in soup.select("a[href*='/2025/'], a[href*='/ua/2025/']"):
        href = a.get("href") or ""
        text = a.get_text(strip=True)
        if not text:
            continue
        if href.startswith("/"):
            href = urljoin(MF_BASE, href)
        # –¥–∞—Ç–∞ –∑ URL (–Ω–∞ minfin —Ü–µ –Ω–∞–¥—ñ–π–Ω—ñ—à–µ, –Ω—ñ–∂ —Ç–µ–∫—Å—Ç)
        date_text = ""
        d = _extract_date_from_url(href)
        if d:
            date_text = d.isoformat()
        # –∫–∞—Ç–µ–≥–æ—Ä—ñ—è –∑ URL
        cat = "news"
        try:
            tail = href.split(MF_BASE)[-1]
            parts = [p for p in tail.split("/") if p]
            if len(parts) >= 2 and parts[0] == "ua":
                cat = parts[2] if len(parts) > 2 else "news"
            elif len(parts) >= 1:
                cat = parts[0]
        except Exception:
            pass
        _add(out, text, href, date_text, "minfin", cat)
        if len(out) >= MAX_PER_PAGE:
            break
    return out

async def _gather_all() -> List[Dict]:
    async with ClientSession() as session:
        ep_all: List[Dict] = []
        for p in EP_PAGES:
            try:
                html = await _fetch(session, urljoin(EP_BASE, p))
                ep_all.extend(_parse_epravda(html, p))
            except Exception as e:
                log.warning(f"epravda fetch fail {p}: {e}")

        mf_all: List[Dict] = []
        for p in MF_PAGES:
            try:
                html = await _fetch(session, urljoin(MF_BASE, p))
                mf_all.extend(_parse_minfin(html, p))
            except Exception as e:
                log.warning(f"minfin fetch fail {p}: {e}")

    # –¥–µ–¥—É–ø –∑–∞ URL
    seen = set()
    items: List[Dict] = []
    for arr in (ep_all, mf_all):
        for it in arr:
            u = it["link"]
            if u not in seen:
                seen.add(u)
                items.append(it)

    # –ª–æ–≥ –¥–æ —Ñ—ñ–ª—å—Ç—Ä–∞
    total_before = len(items)
    log.info(f"üîπ –ü–∞—Ä—Å–∏–º–æ Epravda/Minfin... –í—Å—å–æ–≥–æ (–¥–æ —Ñ—ñ–ª—å—Ç—Ä–∞): {total_before}")

    # —Ñ—ñ–ª—å—Ç—Ä —Ç—ñ–ª—å–∫–∏ ¬´—Å—å–æ–≥–æ–¥–Ω—ñ/–≤—á–æ—Ä–∞¬ª + –≤—ñ–¥—Å—ñ–∫–∞—î–º–æ –±–µ–∑ –¥–∞—Ç–∏
    items = _only_today_yesterday(items)
    total_after = len(items)
    log.info(f"üîπ –ü—ñ—Å–ª—è —Ñ—ñ–ª—å—Ç—Ä–∞ –¥–∞—Ç–∏ (—Å—å–æ–≥–æ–¥–Ω—ñ/–≤—á–æ—Ä–∞): {total_after}")

    # –∫–æ—Ä–æ—Ç–∫–∏–π –∑—Ä—ñ–∑ –ø–æ –¥–∂–µ—Ä–µ–ª–∞—Ö
    def _summ(src: str):
        arr = [x for x in items if x["src"] == src]
        log.info(f"‚úÖ {src} ‚Äî {len(arr)}")

    _summ("epravda")
    _summ("minfin")
    return items

def run_all() -> List[Dict]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ –ª–∏—à–µ –∑–∞ ¬´—Å—å–æ–≥–æ–¥–Ω—ñ/–≤—á–æ—Ä–∞¬ª.
    –ï–ª–µ–º–µ–Ω—Ç:
      title: str
      link: str
      published: YYYY-MM-DD –∞–±–æ –≤–∏—Ö—ñ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç (—è–∫—â–æ –±—É–≤)
      src: 'epravda' | 'minfin'
      category: str
    """
    return asyncio.run(_gather_all())