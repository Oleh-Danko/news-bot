# parsers/minfin_parser.py
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

def parse_minfin():
    base_url = "https://minfin.com.ua"
    sections = [
        "ua/news/",                  # –±—É–ª–æ: "news"
        "ua/news/money-management/",
        "ua/news/commerce/",
        "ua/news/improvement/"
    ]

    all_news = []
    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    target_dates = [today, yesterday]

    per_source = []

    print("üîπ –ü–∞—Ä—Å–∏–º–æ Minfin...")

    for section in sections:
        url = f"{base_url}/{section}"
        section_news = []
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ {url}: {e}")
            continue

        items = soup.select("li.item")

        for item in items:
            date_tag = item.select_one("span.data")
            title_tag = item.select_one("a")
            if not date_tag or not title_tag:
                continue

            title = title_tag.text.strip()
            link = title_tag.get("href", "")
            if not link.startswith("http"):
                link = base_url + link

            raw_date = (
                date_tag.get("content")
                or date_tag.text.strip().replace(" ", " ")
            )

            try:
                news_date = datetime.strptime(raw_date.split()[0], "%Y-%m-%d").date()
            except Exception:
                try:
                    news_date = datetime.strptime(raw_date.split()[0], "%d.%m.%Y").date()
                except Exception:
                    continue

            if news_date not in target_dates:
                continue

            news_item = {
                "title": title,
                "url": link,
                "date": str(news_date),
                "source": url
            }

            section_news.append(news_item)
            all_news.append(news_item)

        per_source.append((url, section_news))

    # –£–Ω—ñ–∫–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–∞ URL
    seen = set()
    unique_news = []
    for news in all_news:
        if news["url"] not in seen:
            unique_news.append(news)
            seen.add(news["url"])

    # –í–∏–≤—ñ–¥ —É —Ñ–æ—Ä–º–∞—Ç—ñ epravda/minfin
    total_with_dups = len(all_news)
    total_unique = len(unique_news)
    print(f"\n‚úÖ minfin - —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"   –£—Å—å–æ–≥–æ –∑–Ω–∞–π–¥–µ–Ω–æ {total_with_dups} (–∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –¥—É–±–ª—ñ–≤)")
    print(f"   –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω: {total_unique}\n")

    for src_url, news_list in per_source:
        print(f"–î–∂–µ—Ä–µ–ª–æ: {src_url} ‚Äî {len(news_list)} –Ω–æ–≤–∏–Ω:")
        for i, n in enumerate(news_list, 1):
            print(f"{i}. {n['title']} ({n['date']})\n   {n['url']}")
        print()

    return unique_news


if __name__ == "__main__":
    try:
        parse_minfin()
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ minfin_parser: {e}")