# parsers/minfin_parser.py
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urlsplit, urlunsplit

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    )
}

BASE_URL = "https://minfin.com.ua"

SECTIONS = [       
    "ua/news/money-management/",    # ‚Üê –≥–æ–ª–æ–≤–Ω–∞ UA-—Å—Ç—Ä—ñ—á–∫–∞ (–∑–∞–º—ñ—Å—Ç—å /news/)
    "ua/news/commerce/",
    "ua/news/improvement/",
    "ua/news/",   
]


def _normalize_url(u: str) -> str:
    """–ü—Ä–∏–≤–æ–¥–∏–º–æ URL –¥–æ –∫–∞–Ω–æ–Ω—ñ—á–Ω–æ–≥–æ –≤–∏–≥–ª—è–¥—É –¥–ª—è –∫–æ—Ä–µ–∫—Ç–Ω–æ—ó —É–Ω—ñ–∫–∞–ª—ñ–∑–∞—Ü—ñ—ó."""
    u = u.strip()
    if not u.startswith("http"):
        u = BASE_URL + u
    s = urlsplit(u)
    netloc = s.netloc.lower()
    path = s.path.rstrip("/")  # –∑—Ä—ñ–∑–∞—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —Å–ª–µ—à
    # –ø—Ä–∏–±–∏—Ä–∞—î–º–æ query —ñ fragment
    return urlunsplit((s.scheme, netloc, path, "", ""))


def _fetch(url: str) -> BeautifulSoup:
    resp = requests.get(url, headers=HEADERS, timeout=20)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")


def parse_minfin():
    all_news = []
    per_source_raw = []  # [(src_url, [items_raw]), ...]

    today = datetime.now().date()
    yesterday = today - timedelta(days=1)
    target_dates = {today, yesterday}

    print("üîπ –ü–∞—Ä—Å–∏–º–æ Minfin...")

    # 1) –ó–±–∏—Ä–∞–Ω–Ω—è –ø–æ –¥–∂–µ—Ä–µ–ª–∞—Ö (RAW, –±–µ–∑ –¥—Ä—É–∫—É)
    for section in SECTIONS:
        src_url = f"{BASE_URL}/{section.strip('/')}/"
        items_raw = []

        try:
            soup = _fetch(src_url)
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ {src_url}: {e}")
            per_source_raw.append((src_url, items_raw))
            continue

        for item in soup.select("li.item"):
            date_tag = item.select_one("span.data")
            title_tag = item.select_one("a")
            if not date_tag or not title_tag:
                continue

            title = title_tag.get_text(strip=True)
            href = title_tag.get("href", "").strip()
            raw_date = (date_tag.get("content") or date_tag.get_text(strip=True) or "").strip()

            # –ü–∞—Ä—Å–∏–º–æ –¥–∞—Ç—É —É —Ñ–æ—Ä–º–∞—Ç–∞—Ö YYYY-MM-DD –∞–±–æ DD.MM.YYYY (–±–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ –¥–∞—Ç—É –±–µ–∑ —á–∞—Å—É)
            news_date = None
            parts0 = raw_date.split()
            if parts0:
                token = parts0[0]
                try:
                    news_date = datetime.strptime(token, "%Y-%m-%d").date()
                except Exception:
                    try:
                        news_date = datetime.strptime(token, "%d.%m.%Y").date()
                    except Exception:
                        news_date = None
            if news_date not in target_dates:
                continue

            url_norm = _normalize_url(href)
            item_obj = {
                "title": title,
                "url": url_norm,
                "date": str(news_date),
                "source": src_url,
            }
            items_raw.append(item_obj)
            all_news.append(item_obj)

        per_source_raw.append((src_url, items_raw))

    # 2) –ì–ª–æ–±–∞–ª—å–Ω–∞ —É–Ω—ñ–∫–∞–ª—ñ–∑–∞—Ü—ñ—è –ø–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–æ–º—É URL
    seen = set()
    unique_news = []
    for news in all_news:
        if news["url"] not in seen:
            unique_news.append(news)
            seen.add(news["url"])

    # 3) –î—Ä—É–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ë–ï–ó –¥—É–±–ª—é–≤–∞–Ω–Ω—è –º—ñ–∂ –¥–∂–µ—Ä–µ–ª–∞–º–∏
    total_with_dups = len(all_news)
    total_unique = len(unique_news)
    print(f"\n‚úÖ minfin - —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"   –£—Å—å–æ–≥–æ –∑–Ω–∞–π–¥–µ–Ω–æ {total_with_dups} (–∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –¥—É–±–ª—ñ–≤)")
    print(f"   –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω: {total_unique}\n")

    printed_urls = set()
    for src_url, items_raw in per_source_raw:
        # –∑–∞–ª–∏—à–∞—î–º–æ –¥–ª—è –¥—Ä—É–∫—É —Ç—ñ–ª—å–∫–∏ —Ç—ñ, —è–∫–∏—Ö —â–µ –Ω–µ –¥—Ä—É–∫—É–≤–∞–ª–∏
        to_print = []
        for it in items_raw:
            if it["url"] in printed_urls:
                continue
            to_print.append(it)
            printed_urls.add(it["url"])

        print(f"–î–∂–µ—Ä–µ–ª–æ: {src_url} ‚Äî {len(to_print)} –Ω–æ–≤–∏–Ω:")
        for i, n in enumerate(to_print, 1):
            print(f"{i}. {n['title']} ({n['date']})\n   {n['url']}")
        print()

    return unique_news


if __name__ == "__main__":
    try:
        parse_minfin()
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ minfin_parser: {e}")