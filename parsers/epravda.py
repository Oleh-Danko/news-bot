#–ó–ê–ì–õ–£–®–ö–ê
def parse_epravda():
    print("‚ÑπÔ∏è –ü–∞—Ä—Å–µ—Ä Epravda –ø–æ–∫–∏ –Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π")
    return []


#–ö–û–î
import requests
from bs4 import BeautifulSoup
from datetime import datetime, date, timedelta
from urllib.parse import urljoin

BASE = "https://www.epravda.com.ua"
SOURCES = [
    "https://www.epravda.com.ua/finances/",
    "https://www.epravda.com.ua/columns/",
]

UA_MONTHS = {
    "—Å—ñ—á–Ω—è": 1, "–ª—é—Ç–æ–≥–æ": 2, "–±–µ—Ä–µ–∑–Ω—è": 3, "–∫–≤—ñ—Ç–Ω—è": 4, "—Ç—Ä–∞–≤–Ω—è": 5, "—á–µ—Ä–≤–Ω—è": 6,
    "–ª–∏–ø–Ω—è": 7, "—Å–µ—Ä–ø–Ω—è": 8, "–≤–µ—Ä–µ—Å–Ω—è": 9, "–∂–æ–≤—Ç–Ω—è": 10, "–ª–∏—Å—Ç–æ–ø–∞–¥–∞": 11, "–≥—Ä—É–¥–Ω—è": 12
}

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    )
}

def _fetch(url: str) -> BeautifulSoup:
    resp = requests.get(url, headers=HEADERS, timeout=20)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")

def _parse_ua_date(text: str) -> date | None:
    """
    –û—á—ñ–∫—É—î —Ä—è–¥–∫–∏ —Ç–∏–ø—É:
    - '27 –∂–æ–≤—Ç–Ω—è, 12:30'
    - '26 –∂–æ–≤—Ç–Ω—è, 18:00'
    –ü–æ–≤–µ—Ä—Ç–∞—î date –∞–±–æ None, —è–∫—â–æ –Ω–µ –≤–¥–∞–ª–æ—Å—å —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏.
    """
    if not text:
        return None
    t = text.strip().lower()
    # –ø—Ä–∏–±–∏—Ä–∞—î–º–æ –≤—Å–µ –ø—ñ—Å–ª—è –∫–æ–º–∏ (—á–∞—Å)
    if "," in t:
        t = t.split(",", 1)[0].strip()
    parts = t.split()
    if len(parts) < 2:
        return None
    try:
        day = int(parts[0])
        month_name = parts[1]
        month = UA_MONTHS.get(month_name)
        if not month:
            return None
        # —Ä—ñ–∫ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ –Ω–µ –≤–∫–∞–∑–∞–Ω–∏–π ‚Äî –±–µ—Ä–µ–º–æ –ø–æ—Ç–æ—á–Ω–∏–π
        year = date.today().year
        return date(year, month, day)
    except Exception:
        return None

def _collect_finances(soup: BeautifulSoup) -> list[dict]:
    """
    –ó finances –±–µ—Ä–µ–º–æ –õ–ò–®–ï —Å—å–æ–≥–æ–¥–Ω—ñ —Ç–∞ –≤—á–æ—Ä–∞.
    –°–µ–ª–µ–∫—Ç–æ—Ä–∏: –±–ª–æ–∫–∏ .article_news (—É –Ω–∏—Ö —î .article_title a —ñ .article_date)
    """
    today = date.today()
    yesterday = today - timedelta(days=1)

    items = []
    for news in soup.select(".article_news"):
        a = news.select_one(".article_title a")
        d = news.select_one(".article_date")
        if not a:
            continue
        title = a.get_text(strip=True)
        url = urljoin(BASE, a.get("href", "").strip())
        date_str = d.get_text(strip=True) if d else ""
        dt = _parse_ua_date(date_str)

        # —Ñ—ñ–ª—å—Ç—Ä: —Ç—ñ–ª—å–∫–∏ —Å—å–æ–≥–æ–¥–Ω—ñ —Ç–∞ –≤—á–æ—Ä–∞
        if dt in (today, yesterday):
            items.append({
                "title": title,
                "url": url,
                "date": dt.strftime("%Y-%m-%d") if dt else "‚Äî",
                "source": "https://epravda.com.ua/finances",
                "section": "finances",
            })
    return items

def _collect_columns(soup: BeautifulSoup) -> list[dict]:
    """
    –ó columns –±–µ—Ä–µ–º–æ –í–°–ï (–¥–∞—Ç –Ω–µ–º–∞—î).
    –¢–∏–ø–æ–≤—ñ –µ–ª–µ–º–µ–Ω—Ç–∏: .article.article_view_sm –∞–±–æ –ø—Ä–æ—Å—Ç–æ .article_title a
    """
    items = []
    # –ü–µ—Ä—à–∏–π –ø—Ä–æ—Ö—ñ–¥: —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω—ñ –∫–∞—Ä—Ç–∫–∏
    for card in soup.select(".article.article_view_sm"):
        a = card.select_one(".article_title a")
        if not a:
            continue
        title = a.get_text(strip=True)
        url = urljoin(BASE, a.get("href", "").strip())
        items.append({
            "title": title,
            "url": url,
            "date": "‚Äî",
            "source": "https://epravda.com.ua/columns",
            "section": "columns",
        })

    # –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –ø—Ä–æ—Ö—ñ–¥: —è–∫—â–æ —Ä–∞–ø—Ç–æ–º —î –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–æ–∑–∞ .article_view_sm
    # (—â–æ–± –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –Ω—ñ—á–æ–≥–æ)
    for a in soup.select(".article_title a"):
        title = a.get_text(strip=True)
        url = urljoin(BASE, a.get("href", "").strip())
        if not any(x["url"] == url for x in items):
            items.append({
                "title": title,
                "url": url,
                "date": "‚Äî",
                "source": "https://epravda.com.ua/columns",
                "section": "columns",
            })

    return items

def parse_epravda() -> list[dict]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–∏–Ω (—Å–ª–æ–≤–Ω–∏–∫–∏) –∑ –¥–≤–æ—Ö –¥–∂–µ—Ä–µ–ª:
    - finances ‚Äî —Ç—ñ–ª—å–∫–∏ —Å—å–æ–≥–æ–¥–Ω—ñ —Ç–∞ –≤—á–æ—Ä–∞,
    - columns ‚Äî —É—Å—ñ.
    –¢–∞–∫–æ–∂ –¥—Ä—É–∫—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É —Å—Ç–∏–ª—ñ Minfin.
    """
    all_found = []
    per_source_print = []

    print("üîπ –ü–∞—Ä—Å–∏–º–æ Epravda...")

    # --- FINANCES ---
    url_fin = SOURCES[0]
    soup_fin = _fetch(url_fin)
    fin_items = _collect_finances(soup_fin)
    per_source_print.append(("https://epravda.com.ua/finances", fin_items))

    # --- COLUMNS ---
    url_col = SOURCES[1]
    soup_col = _fetch(url_col)
    col_items = _collect_columns(soup_col)
    per_source_print.append(("https://epravda.com.ua/columns", col_items))

    # –ó–∞–≥–∞–ª—å–Ω–∏–π —Å–ø–∏—Å–æ–∫ (–∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –¥—É–±–ª—ñ–≤)
    all_found = fin_items + col_items

    # –£–Ω—ñ–∫–∞–ª—å–Ω—ñ—Å—Ç—å –∑–∞ URL
    seen = set()
    unique = []
    for n in all_found:
        if n["url"] not in seen:
            unique.append(n)
            seen.add(n["url"])

    # –í–∏–≤—ñ–¥ –ø–æ –∫–æ–∂–Ω–æ–º—É –¥–∂–µ—Ä–µ–ª—É (—è–∫ —Ç–∏ –ø—Ä–æ—Å–∏–≤)
    total_with_dups = len(all_found)
    total_unique = len(unique)
    print(f"\n‚úÖ epravda - —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"   –£—Å—å–æ–≥–æ –∑–Ω–∞–π–¥–µ–Ω–æ {total_with_dups} (–∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –¥—É–±–ª—ñ–≤)")
    print(f"   –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω: {total_unique}\n")

    global_index = 1
    for src_url, items in per_source_print:
        print(f"–î–∂–µ—Ä–µ–ª–æ: {src_url} ‚Äî {len(items)} –Ω–æ–≤–∏–Ω:")
        for i, n in enumerate(items, 1):
            print(f"{global_index}. {n['title']} ({n['date']})\n   {n['url']}")
            global_index += 1
        print()  # –ø–æ—Ä–æ–∂–Ω—ñ–π —Ä—è–¥–æ–∫ –º—ñ–∂ –¥–∂–µ—Ä–µ–ª–∞–º–∏

    return unique

if __name__ == "__main__":
    try:
        parse_epravda()
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ epravda_parser: {e}")