# parsers/ain_parser.py
import re
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone, date
from email.utils import parsedate_to_datetime
from zoneinfo import ZoneInfo

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    ),
    "Accept": "application/rss+xml, text/xml;q=0.9, */*;q=0.8",
}
KYIV_TZ = ZoneInfo("Europe/Kyiv")

FEED_URL = "https://ain.ua/feed/"
SOURCE_PAGE = "https://ain.ua/"

# üîé –ó–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ —Ç–µ–≥–∏/–∫–∞—Ç–µ–≥–æ—Ä—ñ—ó (—É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é + –º–æ–∂–ª–∏–≤—ñ —Å–ª—É–≥–∏)
BANNED_TEXT = {
    "–ø–æ–ø–Ω–∞—É–∫–∞", "–º—É–∑–∏–∫–∞", "—Å–ø–µ—Ü–ø—Ä–æ—î–∫—Ç–∏", "—Å–ø–µ—Ü–ø—Ä–æ–µ–∫—Ç–∏",
    "pop-science", "music", "special",
}
# üîé –ó–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ –ø—Ä–µ—Ñ—ñ–∫—Å–∏ —à–ª—è—Ö—ñ–≤ –ø–æ—Å–∏–ª–∞–Ω—å –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ
BANNED_PREFIXES = ("/pop-science/", "/pop-science/music/", "/special/")

def _fetch_text(url: str) -> str:
    resp = requests.get(url, headers=HEADERS, timeout=25)
    resp.raise_for_status()
    return resp.text

def _to_kyiv_date(pubdate_text: str) -> date | None:
    if not pubdate_text:
        return None
    try:
        dt = parsedate_to_datetime(pubdate_text)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(KYIV_TZ).date()
    except Exception:
        m = re.search(r"\d{4}-\d{2}-\d{2}", pubdate_text)
        if m:
            try:
                return datetime.strptime(m.group(0), "%Y-%m-%d").date()
            except Exception:
                return None
        return None

def _has_banned_category_text(categories: list[str]) -> bool:
    for c in categories:
        lc = (c or "").strip().lower()
        if not lc:
            continue
        # —è–∫—â–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è –º—ñ—Å—Ç–∏—Ç—å –±—É–¥—å-—è–∫–∏–π —ñ–∑ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏—Ö –∫–ª—é—á—ñ–≤
        if any(bt in lc for bt in BANNED_TEXT):
            return True
    return False

def _page_has_banned_tag(url: str) -> bool:
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ç–µ–≥-–º–µ–Ω—é —Å—Ç–∞—Ç—Ç—ñ (.widget__header_tags)."""
    try:
        html = _fetch_text(url)
        soup = BeautifulSoup(html, "html.parser")
        for a in soup.select(".widget__header_tags a[href]"):
            href = (a.get("href") or "").strip()
            text = (a.get_text(strip=True) or "").lower()
            if any(href.startswith(p) for p in BANNED_PREFIXES):
                return True
            if any(bt in text for bt in BANNED_TEXT):
                return True
    except Exception:
        # –Ø–∫—â–æ —Å—Ç–æ—Ä—ñ–Ω–∫—É –Ω–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ ‚Äî –Ω–µ –±–ª–æ–∫—É—î–º–æ –ø–æ–º–∏–ª–∫–æ–≤–æ
        return False
    return False

def parse_ain() -> list[dict]:
    print("üîπ –ü–∞—Ä—Å–∏–º–æ AIN.ua...")

    today = datetime.now(tz=KYIV_TZ).date()
    yesterday = today - timedelta(days=1)
    target_dates = {today, yesterday}

    xml_text = _fetch_text(FEED_URL)
    root = ET.fromstring(xml_text)

    items_raw = []
    for item in root.findall(".//item"):
        title_el = item.find("title")
        link_el = item.find("link")
        pub_el = item.find("pubDate")
        cats_el = item.findall("category")

        title = (title_el.text or "").strip() if title_el is not None else ""
        url = (link_el.text or "").strip() if link_el is not None else ""
        pubdate = (pub_el.text or "").strip() if pub_el is not None else ""
        categories = [(c.text or "").strip() for c in cats_el if c is not None]

        d = _to_kyiv_date(pubdate)
        if not d or d not in target_dates:
            continue

        # 1) –§—ñ–ª—å—Ç—Ä –ø–æ RSS-–∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º
        if _has_banned_category_text(categories):
            continue

        # 2) –î–æ–¥–∞—Ç–∫–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–∞–º–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–∞ –∑–∞–±–æ—Ä–æ–Ω–µ–Ω—ñ —Ç–µ–≥–∏
        if _page_has_banned_tag(url):
            continue

        items_raw.append(
            {
                "title": title,
                "url": url,
                "date": d.strftime("%Y-%m-%d"),
                "source": SOURCE_PAGE,
                "section": "ain.ua",
            }
        )

    # –î–µ–¥—É–ø –∑–∞ URL
    seen = set()
    unique = []
    for n in items_raw:
        if n["url"] in seen:
            continue
        seen.add(n["url"])
        unique.append(n)

    print("\n‚úÖ ain.ua - —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"   –£—Å—å–æ–≥–æ –∑–Ω–∞–π–¥–µ–Ω–æ {len(items_raw)} (–∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –¥—É–±–ª—ñ–≤)")
    print(f"   –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω: {len(unique)}\n")

    print(f"üü¢–î–∂–µ—Ä–µ–ª–æ: {SOURCE_PAGE} ‚Äî {len(unique)} –Ω–æ–≤–∏–Ω:")
    for i, n in enumerate(unique, 1):
        print(f"{i}. {n['title']} ({n['date']})\n   {n['url']}")
    print()

    return unique

if __name__ == "__main__":
    try:
        parse_ain()
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ ain_parser: {e}")