# parsers/coindesk_parser.py
import re
from datetime import date, datetime, timedelta
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

BASE = "https://www.coindesk.com"
SOURCE_URL = "https://www.coindesk.com/uk/latest-crypto-news"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    )
}

def _fetch(url: str) -> BeautifulSoup:
    resp = requests.get(url, headers=HEADERS, timeout=20)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")

def _abs(url: str) -> str:
    if url.startswith("http"):
        return url
    return urljoin(BASE, url)

def _extract_date_from_url(u: str) -> date | None:
    m = re.search(r"/(\d{4})/(\d{2})/(\d{2})/", u)
    if not m:
        return None
    try:
        y, mth, d = map(int, m.groups())
        return date(y, mth, d)
    except Exception:
        return None

def _best_title(a_tag) -> str:
    # 1) –∑–∞–≥–æ–ª–æ–≤–æ–∫ —É <a>, 2) –∑–∞–≥–æ–ª–æ–≤–æ–∫ —É –Ω–∞–π–±–ª–∏–∂—á–∏—Ö h2/h3/h4 –ø–æ—Ä—É—á
    t = (a_tag.get_text(strip=True) or "").strip()
    if t:
        return t
    for parent in (a_tag, a_tag.parent, getattr(a_tag, "parent", None).parent if a_tag and a_tag.parent else None):
        if not parent:
            continue
        h = parent.find(["h2", "h3", "h4"])
        if h:
            tt = h.get_text(strip=True)
            if tt:
                return tt
    return ""

def parse_coindesk() -> list[dict]:
    print("üîπ –ü–∞—Ä—Å–∏–º–æ CoinDesk...")

    soup = _fetch(SOURCE_URL)
    today = date.today()
    yesterday = today - timedelta(days=1)
    target = {today, yesterday}

    seen_urls = set()
    items: list[dict] = []

    # –ë–µ—Ä–µ–º–æ –≤—Å—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è —ñ —Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ —Ç—ñ–ª—å–∫–∏ —Å—Ç–∞—Ç—Ç—ñ /uk/.../YYYY/MM/DD/...
    for a in soup.select('a[href]'):
        href = a.get("href", "").strip()
        if not href:
            continue
        url = _abs(href)

        # —Ç—ñ–ª—å–∫–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞ —Å–µ–∫—Ü—ñ—è —Ç–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏-—Å—Ç–∞—Ç—Ç—ñ –∑ –¥–∞—Ç–æ—é –≤ URL
        if "/uk/" not in url:
            continue
        if SOURCE_URL.rstrip("/") == url.rstrip("/"):
            continue

        dt = _extract_date_from_url(url)
        if not dt or dt not in target:
            continue

        title = _best_title(a)
        if not title:
            continue

        if url in seen_urls:
            continue
        seen_urls.add(url)

        items.append({
            "title": title,
            "url": url,
            "date": dt.strftime("%Y-%m-%d"),
            "source": SOURCE_URL,
            "section": "coindesk-uk",
        })

    # –°–æ—Ä—Ç—É—î–º–æ –≤—ñ–¥ –Ω–æ–≤—ñ—à–∏—Ö –¥–æ —Å—Ç–∞—Ä—ñ—à–∏—Ö
    items.sort(key=lambda x: (x["date"], x["title"]), reverse=True)

    # –í–∏–≤—ñ–¥, —è–∫ —É —Ä–µ—à—Ç—ñ –ø–∞—Ä—Å–µ—Ä—ñ–≤
    print("\n‚úÖ coindesk - —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"   –£—Å—å–æ–≥–æ –∑–Ω–∞–π–¥–µ–Ω–æ {len(items)} (–∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –¥—É–±–ª—ñ–≤)")
    print(f"   –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω: {len(items)}\n")

    print(f"–î–∂–µ—Ä–µ–ª–æ: {SOURCE_URL} ‚Äî {len(items)} –Ω–æ–≤–∏–Ω:")
    for i, n in enumerate(items, 1):
        print(f"{i}. {n['title']} ({n['date']})\n   {n['url']}")
    print()

    return items

if __name__ == "__main__":
    try:
        parse_coindesk()
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ coindesk_parser: {e}")