# parsers/epravda_parser.py
import requests
from bs4 import BeautifulSoup
from datetime import date, timedelta
from urllib.parse import urljoin

BASE = "https://www.epravda.com.ua"
FINANCES_URL = "https://www.epravda.com.ua/finances/"

UA_MONTHS = {
    "—Å—ñ—á–Ω—è": 1, "–ª—é—Ç–æ–≥–æ": 2, "–±–µ—Ä–µ–∑–Ω—è": 3, "–∫–≤—ñ—Ç–Ω—è": 4, "—Ç—Ä–∞–≤–Ω—è": 5, "—á–µ—Ä–≤–Ω—è": 6,
    "–ª–∏–ø–Ω—è": 7, "—Å–µ—Ä–ø–Ω—è": 8, "–≤–µ—Ä–µ—Å–Ω—è": 9, "–∂–æ–≤—Ç–Ω—è": 10, "–ª–∏—Å—Ç–æ–ø–∞–¥–∞": 11, "–≥—Ä—É–¥–Ω—è": 12
}

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/126.0.0.0 Safari/537.36"
    )
}

def _fetch(url: str) -> BeautifulSoup:
    resp = requests.get(url, headers=HEADERS, timeout=20)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, "html.parser")

def _parse_ua_date(text: str) -> date | None:
    if not text:
        return None
    t = text.strip().lower()
    if "," in t:
        t = t.split(",", 1)[0].strip()
    parts = t.split()
    if len(parts) < 2:
        return None
    try:
        day = int(parts[0])
        month = UA_MONTHS.get(parts[1])
        if not month:
            return None
        return date(date.today().year, month, day)
    except Exception:
        return None

def _collect_finances(soup: BeautifulSoup) -> list[dict]:
    today = date.today()
    yesterday = today - timedelta(days=1)
    items = []
    for news in soup.select(".article_news"):
        a = news.select_one(".article_title a")
        d = news.select_one(".article_date")
        if not a:
            continue
        title = a.get_text(strip=True)
        url = urljoin(BASE, a.get("href", "").strip())
        date_str = d.get_text(strip=True) if d else ""
        dt = _parse_ua_date(date_str)
        if dt in (today, yesterday):
            items.append({
                "title": title,
                "url": url,
                "date": dt.strftime("%Y-%m-%d"),
                "source": "https://epravda.com.ua/finances",
                "section": "finances",
            })
    return items

def parse_epravda() -> list[dict]:
    print("üîπ –ü–∞—Ä—Å–∏–º–æ Epravda...")

    soup_fin = _fetch(FINANCES_URL)
    fin_items = _collect_finances(soup_fin)

    all_found = fin_items
    seen = set()
    unique = []
    for n in all_found:
        if n["url"] not in seen:
            unique.append(n)
            seen.add(n["url"])

    print("\n‚úÖ epravda - —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"   –£—Å—å–æ–≥–æ –∑–Ω–∞–π–¥–µ–Ω–æ {len(all_found)} (–∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –¥—É–±–ª—ñ–≤)")
    print(f"   –£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –Ω–æ–≤–∏–Ω: {len(unique)}\n")

    print(f"–î–∂–µ—Ä–µ–ª–æ: https://epravda.com.ua/finances ‚Äî {len(fin_items)} –Ω–æ–≤–∏–Ω:")
    for i, n in enumerate(fin_items, 1):
        print(f"{i}. {n['title']} ({n['date']})\n   {n['url']}")
    print()

    return unique

if __name__ == "__main__":
    try:
        parse_epravda()
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ epravda_parser: {e}")